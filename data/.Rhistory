{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r <- abs(cor(x, y))
txt <- format(c(r, 0.123456789), digits = digits)[1]
txt <- paste0(prefix, txt)
if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(Boston, col="#48D1CC",lower.panel = panel.smooth, upper.panel = panel.cor, gap=0, row1attop=FALSE)
#15b
lm = lm(crim~., data=Boston)
summary(lm)
# For zn, dis, rad, black, medv we can reject the null hypothesis that coefficient is 0 as Pr(>|t|) < 0.05 value
x = c(coefficients(lm.zn)[2],
coefficients(lm.indus)[2],
coefficients(lm.chas)[2],
coefficients(lm.nox)[2],
coefficients(lm.rm)[2],
coefficients(lm.age)[2],
coefficients(lm.dis)[2],
coefficients(lm.rad)[2],
coefficients(lm.tax)[2],
coefficients(lm.ptratio)[2],
coefficients(lm.black)[2],
coefficients(lm.lstat)[2],
coefficients(lm.medv)[2])
y = coefficients(lm)[2:14]
m=data.table(x,y)
g8<-ggplot(m, aes(x=x ,y=y))+labs(title="Coefficient multivariate v/s Linear ")+geom_point(col="#48D1CC")+theme_calc()+ylab("Multivariate")+ xlab("Linear")+geom_abline(slope=1)
plot(g8)
#15c
lm.zn = lm(crim~poly(zn,3))
summary(lm.zn) # 1, 2
lm.indus = lm(crim~poly(indus,3))
summary(lm.indus) # 1, 2, 3
lm.nox = lm(crim~poly(nox,3))
summary(lm.nox) # 1, 2, 3
lm.rm = lm(crim~poly(rm,3))
summary(lm.rm) # 1, 2
lm.age = lm(crim~poly(age,3))
summary(lm.age) # 1, 2, 3
lm.dis = lm(crim~poly(dis,3))
summary(lm.dis) # 1, 2, 3
lm.rad = lm(crim~poly(rad,3))
summary(lm.rad) # 1, 2
lm.tax = lm(crim~poly(tax,3))
summary(lm.tax) # 1, 2
lm.ptratio = lm(crim~poly(ptratio,3))
summary(lm.ptratio) # 1, 2, 3
lm.black = lm(crim~poly(black,3))
summary(lm.black) # 1
lm.lstat = lm(crim~poly(lstat,3))
summary(lm.lstat) # 1, 2
lm.medv = lm(crim~poly(medv,3))
summary(lm.medv) # 1, 2, 3
## Chapter 6
########################################################################################################################################
#9a
library(ISLR)
set.seed(10)
library(data.table)
data_3<-data.table(College)
s=nrow(data_3)*0.8
train = sample(1:nrow(data_3), size=s)
College.train = College[train, ]
College.test = College[-train, ]
#9b
lm = lm(Apps~., data=College.train)
lm.pred = predict(lm, College.test)
mean((College.test[, "Apps"] - lm.pred)^2)
x=c(1:5)
x[1]<-mean((College.test[, "Apps"] - lm.pred)^2)
library(glmnet)
train.mat = model.matrix(Apps~., data=College.train)
test.mat = model.matrix(Apps~., data=College.test)
grid = 10 ^ seq(4, -2, length=100)
ridge = cv.glmnet(train.mat, College.train[, "Apps"], alpha=0, lambda=grid, thresh=1e-12)
lambda.best = ridge$lambda.min
lambda.best
ridge.pred = predict(ridge, newx=test.mat, s=lambda.best)
mean((College.test[, "Apps"] - ridge.pred)^2)
x[2]<-mean((College.test[, "Apps"] - ridge.pred)^2)
lasso = cv.glmnet(train.mat, College.train[, "Apps"], alpha=1, lambda=grid, thresh=1e-12)
lambda.best = lasso$lambda.min
lambda.best
lasso.pred = predict(lasso, newx=test.mat, s=lambda.best)
x[3]<-mean((College.test[, "Apps"] - lasso.pred)^2)
lasso = glmnet(model.matrix(Apps~., data=College), College[, "Apps"], alpha=1, lambda=9.32)
predict(lasso, s=lambda.best, type="coefficients")
#9e
####Principal Component Regression
library(pls)
pcr= pcr(Apps~., data=College.train, scale=T, validation="CV")
validationplot(pcr, val.type="MSEP", col="#48D1CC")
pcr$ncomp
pcr.pred=predict(pcr,newdata = College.test,ncomp = 17)
mean((College.test[, "Apps"] - pcr.pred)^2)
x[4]<-mean((College.test[, "Apps"] - pcr.pred)^2)
#### Partial Least Squares Regression
pls= plsr(Apps~., data=College.train, scale=T, validation="CV")
validationplot(pls, val.type="MSEP", col="#48D1CC")
pls$ncomp
pls.pred = predict(pls, College.test, ncomp=17)
mean((College.test[, "Apps"] - pls.pred)^2)
x[5]<-mean((College.test[, "Apps"] - pls.pred)^2)
#9g
print(x)
#The order for above MSE is least squares, ridge, lasso, principal component regression and partial least squares. There is not much difference in the errors for the 5 algorithms. But Lasso Regression still has the least MSE.
#11a
#Subset Selection
library(glmnet)
library(leaps)     # For Subset Selection
library(pls)       # For PCR
library(MASS)
set.seed(11)
attach(Boston)
predict.regsubsets = function(regsubset, new, id, ...) {form = as.formula(regsubset$call[[2]])
mat = model.matrix(form, new)
coefi = coef(regsubset, id = id)
mat[, names(coefi)] %*% coefi}
n=nrow(Boston)
folds = sample(rep(1:10, length = nrow(Boston)))
cv.errors = matrix(NA, 10,13)
for (i in 1:10) {
regsubset = regsubsets(crim ~ ., data = Boston[folds != i, ], nvmax = n)
for (j in 1:13) {
pred = predict(regsubset, Boston[folds == i, ], id = j)
cv.errors[i, j] = mean((Boston$crim[folds == i] - pred)^2)
}}
#13 is the number of coulmns minus 1 in the dataset
rmse.cv = sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, pch = 19, type = "b", col="#48D1CC")
which.min(rmse.cv)
rmse.cv[9]
#Minimum RMSE occurs at 9 variables and the minimum RMSE is 6.61 @ 9 variables
#Lasso
set.seed(11)
x = model.matrix(crim ~ . - 1, data = Boston)
cv.lasso = cv.glmnet(x, Boston$crim, type.measure = "mse")
plot(cv.lasso, pch = 19, type = "b",col="#48D1CC")
coef(cv.lasso)
sqrt(cv.lasso$cvm[cv.lasso$lambda == cv.lasso$lambda.1se])
#The MSE for Lasso method is 7.70 (higher than subset slection)
#Ridge Regression
set.seed(9)
x = model.matrix(crim ~ . - 1, data = Boston)
y = Boston$crim
cv.R = cv.glmnet(x, y, type.measure = "mse", alpha = 0)
plot(cv.R)
coef(cv.R)
sqrt(cv.R$cvm[cv.R$lambda == cv.R$lambda.1se])
# The MSE for Ridge method is 7.70 (higher than subset slection and close to Lasso)
library(pls)
set.seed(9)
pcr.fit = pcr(crim ~ ., data = Boston, scale = TRUE, validation = "CV")
summary(pcr.fit)
plot(pcr.fit, col=)
#The 13 component PCR fit has the lowest cross validation error (adjCV = 6.56)
#9b
# Among all the models tried above, the PCR has the lowest CV error at 13 components on test set. The error in case of subset selection closely follows PCR with 6.61 RMSE at 9 components again. Subset selection with 9 variables gives almost the same RMSE as with 13 variables PCR. I would suggest subset selected model with 9 varaibles because of two reasons:
#
# 1. Comparative RMSE w.r.t.PCR with least RMSE
# 2. Less number of variables and hence low complexity
#9c
# Choosen model which is subset selection doesn't involve all features as we selected the one with lower complexity and comparative RMSE.
##Chapter 4
########################################################################################################################################
#10a
library(ISLR)
attach(Weekly)
head(Weekly)
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
usr <- par("usr"); on.exit(par(usr))
par(usr = c(0, 1, 0, 1))
r <- abs(cor(x, y))
txt <- format(c(r, 0.123456789), digits = digits)[1]
txt <- paste0(prefix, txt)
if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
text(0.5, 0.5, txt, cex = cex.cor * r)
}
pairs(Weekly, col="#48D1CC",lower.panel = panel.smooth, upper.panel = panel.cor, gap=0, row1attop=FALSE)
#10b
attach(Weekly)
glm.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(glm.fit)
library(caret)
glm.prob = predict(glm.fit, type = "response")
glm.pred = ifelse(glm.prob>0.5, "Up","Down")
confusionMatrix(as.factor(glm.pred), Direction)
train = (Year < 2009)
Weekly_test = Weekly[!train, ]
glm.fit = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit, Weekly_test, type = "response")
glm.pred = ifelse(glm.probs>0.5, "Up", "Down")
Direction_test = Weekly_test[,'Direction']
confusionMatrix(as.factor(glm.pred),as.factor( Direction_test))
#The overall fraction of correct predictions is 62.5% (fraction of correct ups and downs out of the hold sample)
#10g
library(class)
# For using Knn
train.knn = as.matrix(Lag2[train])
test.knn = as.matrix(Lag2[!train])
train.Direction = Direction[train]
set.seed(100)
knn.pred = knn(train.knn, test.knn, train.Direction, k = 1)
confusionMatrix(knn.pred, Direction_test)
#The overall fraction of correct predictions is 50%
#10i
# Logistic regression with Lag1, Lag2 and Lag4 as seen from multivariate regression
glm.fit = glm(Direction ~ Lag2+Lag1+Lag4, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit, Weekly_test, type = "response")
glm.pred = ifelse(glm.probs > 0.5, "Up", "Down")
Direction_test = Direction[!train]
summary(glm.fit)
confusionMatrix(as.factor(glm.pred),as.factor( Direction_test))
#The accuracy is 60.58%
# Logistic regression with Lag1, Lag2 from previous model
glm.fit = glm(Direction ~ Lag2+Lag1, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit, Weekly_test, type = "response")
glm.pred = ifelse(glm.probs > 0.5, "Up", "Down")
Direction_test = Direction[!train]
summary(glm.fit)
confusionMatrix(as.factor(glm.pred),as.factor( Direction_test))
#The accuracy here drops to 57.69%
set.seed(9)
knn.pred = knn(train.knn, test.knn, train.Direction, k = 20)
confusionMatrix(knn.pred, Direction_test)
#The accuracy here drops to 57.69%
set.seed(9)
knn.pred = knn(train.knn, test.knn, train.Direction, k = 100)
confusionMatrix(knn.pred, Direction_test)
#Accuracy of 56.73%
set.seed(11)
out_MSE = matrix(0,1,100)
for (k in 1:100){
KNN_model_pred_k = knn(train.knn, test.knn, train.Direction, k = k)
Direction_test=Direction[!train]
table(KNN_model_pred_k, Direction_test)
aux=matrix(mean(KNN_model_pred_k==Direction_test))
out_MSE[,k] = aux}
out_MSE[which.max(out_MSE)]
#At k=80, the accuracy rate is 62.5%
########################################################################################################################################
### Chapter 8
#8a
#install.packages("ISLR") to get the data set
library(ISLR)
data_6<-Carseats
attach(Carseats)
set.seed(80) #to ensure consistent sampling
train = sample(1:nrow(data_6),0.8*nrow(data_6))
Carseats.train = data_6[train, ]
Carseats.test = data_6[-train, ]
#8b
set.seed(10)
library(tree)
tree.carseats = tree(Sales ~ ., data = Carseats.train, mindev=0.009)
summary(tree.carseats)
#The data pertains to sales of child car seats. The variables used in the final tree model are "ShelveLoc", "Price", "Age", "CompPrice", "Population" and "Advertising". The number of nodes of the tree are 19 with the residual mean deviance of 2.553.
library(ggplot2)
library(tree)
plot(tree.carseats,type = "uniform")
text(tree.carseats, cex=.6,adj = par("adj"), pretty=0, col="#48D1CC")
#The Tree has ShelveLoc as it's root node, If shelveloc is Bad, the decision then moves to left node, Price if Price is less than 105.5, we move to the left node for the decision. At each node whenever the condition is fulfilled we move to the left node. Here we are predictiong sales of the carseats and at the end of the tree we have 19 terminal nodes which predict the sales of the tree on the basis of the conditions in the intermediate nodes. To make prediction for a x in validation or test set, we drop the x on the tree and after going through each node the terminal node at which it lands is the predcition for it's sale.
set.seed(10)
pred = predict(tree.carseats, Carseats.test)
mean((Carseats.test$Sales - pred)^2)
#The test set MSE of the predictions comes out to be 3.76.
#8c
set.seed(10)
cv = cv.tree(tree.carseats, FUN = prune.tree)
par(mfrow=c(1,2))
plot(cv$size, cv$dev, type = "b", col ="#48D1CC")
plot(cv$k, cv$dev, type = "b", col ="#48D1CC", xlim=c(10,100))
#summary(cv) #taking best size
#The minimum deviation occurs at number of trees = 14
#The size of cross validation set with minimum MSE is close to 30
#Best size = 14
pruned = prune.tree(tree.carseats, best = 14)
plot(pruned, type = "uniform")
text(pruned, pretty = 0, col="#48D1CC", cex=0.6)
pred.pruned = predict(pruned, Carseats.test)
mean((Carseats.test$Sales - pred.pruned)^2)
#The MSE is slightly lower now than the actual tree splitup with 19 nodes.
#8d
library(randomForest)
set.seed(10)
rf.bag = randomForest(Sales ~ ., data = data_6, ntree = 600, importance = T)
pred.rfbag = predict(rf.bag, Carseats.test)
mean((Carseats.test$Sales - pred.rfbag)^2)
Obtained an MSE of 0.40 way below the previous values. Implemented random forest with all variables so that bagging can occur (no subsetting)
importance(rf.bag)
#ShelveLoc and Price are the most important predictors of Sale followed by Age, Advertising and CompPrice.
#11e
#library(randomForest)
mse=c()
set.seed(42)
for(i in 3:10){
rf=randomForest(Sales~.,data=Carseats.train,importance=T,ntree=200)
pred.rf=predict(rf,Carseats.test)
mse=rbind(mse,mean((pred.rf-Carseats.test$Sales)^2))
}
par(mfrow=c(1,2))
plot(3:10,mse,type='b',col ="#48D1CC")
plot(rf, col ="#48D1CC")
#The error saturates after number of trees ~ 150, the MSE at each iteration has been plotted in the left plot.
mean(mse)
importance(rf, col ="#48D1CC")
Again important variables are: Price, ShelveLoc, CompPrice and age. The error saturates at about 150 trees (visual inspection)
Obtained an MSE of 2.07 from all random forest iterations => (3,10)
set.seed(10)
rf_1=randomForest(Sales~.,data=Carseats.train, test= Carseats.test,mtry=2,importance=T,ntree=150)
rf_2=randomForest(Sales~.,data=Carseats.train, mtry=3,test= Carseats.test,importance=T,ntree=150)
rf_3=randomForest(Sales~.,data=Carseats.train, mtry=4,test= Carseats.test,importance=T,ntree=150)
print(rf_1)
print(rf_2)
print(rf_3)
#The error rate decreses if m the number of variables considered at each split are increased. From 3.69 MSE at 2 to 2.72 MSE at 4 variables.
#11a
library(ISLR)
library(data.table)
data_7<-data.table(Caravan)
data_7$Purchase = ifelse(data_7$Purchase == "Yes", 1, 0)
train = 1:1000
Caravan.train = data_7[train, ]
Caravan.test = data_7[-train, ]
#The data contains 5822 real customer records. Each record consists of 86 variables, containing sociodemographic data (variables 1-43) and product ownership (variables 44-86). The sociodemographic data is derived from zip codes. All customers living in areas with the same zip code have the same sociodemographic attributes. Variable 86 (Purchase) indicates whether the customer purchased a caravan insurance policy.
#11b
library(gbm)
set.seed(10)
GBM= gbm(Purchase ~., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, distribution ="bernoulli")
summary(GBM)
#PPERSAUT, MKOOPKLA and MOPLHOOG appear as the most significant in the GBM technique
#11c
set.seed(10)
boost.prob = predict(GBM, Caravan.test, n.trees = 1000, type = "response")
boost.pred = ifelse(boost.prob > 0.2, 1, 0)
library(caret)
confusionMatrix(as.factor(Caravan.test$Purchase), as.factor(boost.pred))
#About 22.30% of people predicted to make purchase actually end up making one.
lm = glm(Purchase ~ ., data = Caravan.train, family = binomial)
lm.prob = predict(lm, Caravan.test, type = "response")
lm.pred = ifelse(lm.prob > 0.2, 1, 0)
confusionMatrix(as.factor(Caravan.test$Purchase), as.factor(lm.pred))
#In case of boosting 22% of the people who make purchase can be correctly predicted. For logisitic the same percentage drops to 14%
library(class)
library(caret)
train.knn = as.matrix(Caravan.train[,-Purchase])
test.knn = as.matrix(Caravan.test[,-Purchase])
train.Purchase = Caravan.train$Purchase
test.Purchase = Caravan.test$Purchase
set.seed(100)
knn.pred = knn(train.knn, test.knn, train.Purchase, k = 10)
confusionMatrix(as.factor(knn.pred),as.factor(test.Purchase))
########################################################################################################################################
##Problem 1: Beauty Pays!
library(ggplot2)
g=ggplot(data=data_7,aes(y=data_7$CourseEvals,x=data_7$BeautyScore))+geom_point(col ="#48D1CC")+geom_smooth(col="#D35400")
plot(g)
par(mfrow=c(2,2))
boxplot(data_7$CourseEvals,data_7$female, col="#48D1CC")
boxplot(data_7$CourseEvals,data_7$lower, col="#48D1CC")
boxplot(data_7$CourseEvals,data_7$nonenglish, col="#48D1CC")
boxplot(data_7$CourseEvals,data_7$tenuretrack, col="#48D1CC")
Modelling
#Linear regression
lm_b = lm(CourseEvals~.,data=data_7)
y=predict(lm_b)
summary(lm_b)
########################################################################################################################################
##Problem 2: Housing Price Structure
data_8m<-read.csv(file="MidCity.csv")
attach(data_8m)
data_8<-data_8m[,-1]
data_8$Nbhd<-as.factor(data_8$Nbhd)
lm=lm(Price~., data=data_8)
summary(lm)
data_8$Nbhd<-as.factor(data_8$Nbhd)
lm=lm(Price~.+Nbhd*Brick, data=data_8)
summary(lm)
data_8['NbhdNew']=ifelse(data_8$Nbhd =='3', "New", "Old")
data_8$NbhdNew=as.factor(data_8$NbhdNew)
lm=lm(Price~.-1, data=data_8)
summary(lm)
data_8['NbhdNew']=ifelse(data_8$Nbhd =='3', "New", "Old")
data_8$NbhdNew=as.factor(data_8$NbhdNew)
lm=lm(Price~.-Nbhd, data=data_8)
summary(lm)
########################################################################################################################################
##Problem 4: BART
Apply BART to the California Housing data and compare with random forest and boosting
set.seed(99)
library(BART) #BART package
library(randomForest)
library(gbm)
data_10<-read.csv("CAhousing.csv")
head(data_10)
nd=200 # number of kept draws
burn=50 # number of burn in draws
x = data_10[,1:8]
y = data_10$medianHouseValue
n=length(y) #total sample size
set.seed(14) #
ii = sample(1:n,floor(.75*n)) # indices for train data, 75% of data
xtrain=x[ii,]; ytrain=y[ii] # training data
xtest=x[-ii,]; ytest=y[-ii] # test data
#BART
bf = wbart(x,y,nskip=burn,ndpost=nd)
yhat = predict(bf, xtest)
#Linear regression
lmf = lm(y~.,data.frame(x,y))
n=length(y) #total sample size
#Random Forest
rf=randomForest(medianHouseValue~.,data=data_10,ntree=10)
y_rf=predict(rf)
#Gradient Boosting
boost = gbm(y~.,data=data_10,distribution='gaussian', interaction.depth=2,n.trees=10,shrinkage=.2)
fmat = predict(boost,n.trees=10)
y_rf[is.na(y_rf)]<-0
fitmat = cbind(y,bf$yhat.train.mean,lmf$fitted.values,y_rf,fmat)
colnames(fitmat)=c("y","BART","Linear","Random Forest","Boost")
cor(fitmat)
yhat.mean = apply(yhat,2,mean)
error=sqrt(sum((ytest-yhat.mean)^2))
#The boosting method is more correlated to the actual values and hence a better predictor in theory though we should not ignore the overfitting problems. BART follows and then Random forest, all our actually trees based on either bagging or boosting.
########################################################################################################################################
##Problem 5: Neural Nets
###### Single Layer Neural Net
set.seed(100)
library(nnet)
library(MASS)
attach(Boston)
summary(Boston)
###standardize
minv = rep(0,14)
maxv = rep(0,14)
Boston$chas=as.numeric(Boston$chas)
Bostonsc = Boston
for(i in 1:14) {
minv[i] = min(Boston[[i]])
maxv[i] = max(Boston[[i]])
Bostonsc[[i]] = (Boston[[i]]-minv[i])/(maxv[i]-minv[i])
}
### nn library
###fit nn with just one x=lstat
set.seed(99)
znn = nnet(crim~lstat,Bostonsc,size=4,decay=.1,linout=T)
###get fits, print summary and plot fit
fznn = predict(znn,Bostonsc)
oo = order(Bostonsc$lstat)
{plot(Bostonsc$lstat,Bostonsc$crim, col="#48D1CC")
lines(Bostonsc$lstat[oo],fznn[oo],col="red",lwd=2)
abline(lm(crim~lstat,Bostonsc)$coef)}
summary(znn)
### trying 6 units
znn = nnet(crim~lstat,Bostonsc,size=6,decay=.3,linout=T)
print(summary(znn))
### all x's
znn = nnet(crim~.,Bostonsc,size=10,decay=.3,linout=T)
fznn = predict(znn,Bostonsc)
zlm = lm(crim~.,Bostonsc)
fzlm = predict(zlm,Bostonsc)
temp = data.frame(y=Bostonsc$crim,fnn=fznn,flm=fzlm)
pairs(temp, col="#48D1CC")
print(cor(temp))
## Size and Decay
znn1 = nnet(crim~lstat,Bostonsc,size=3,decay=.5,linout=T)
znn2 = nnet(crim~lstat,Bostonsc,size=4,decay=.00002,linout=T)
znn3 = nnet(crim~lstat,Bostonsc,size=40,decay=.5,linout=T)
znn4 = nnet(crim~lstat,Bostonsc,size=50,decay=.0001,linout=T)
temp = data.frame(crim = Bostonsc$crim, lstat = Bostonsc$lstat)
znnf1 = predict(znn1,temp)
znnf2 = predict(znn2,temp)
znnf3 = predict(znn3,temp)
znnf4 = predict(znn4,temp)
### plot the fits
{par(mfrow=c(2,2))
plot(Bostonsc$lstat,Bostonsc$crim,col="#48D1CC")
lines(Bostonsc$lstat[oo],znnf1[oo],col="red",lwd=2)
title("size=3, decay=.5")
plot(Bostonsc$lstat,Bostonsc$crim,col="#48D1CC")
lines(Bostonsc$lstat[oo],znnf2[oo],col="red",lwd=2)
title("size=3, decay=.00001")
plot(Bostonsc$lstat,Bostonsc$crim,col="#48D1CC")
lines(Bostonsc$lstat[oo],znnf3[oo],col="red",lwd=2)
title("size = 50, decay = .5")
plot(Bostonsc$lstat,Bostonsc$crim,col="#48D1CC")
lines(Bostonsc$lstat[oo],znnf4[oo],col="red",lwd=2)
title("size = 50, decay = .00001")}
#The neural network performs only slightly better than the linear regression as seen from the correlation values.
setwd("C:/Users/Avani/Downloads/R_Predictive_Modelling_Part2/STA380-master/STA380-master/data")
setwd("C:/Users/Avani/Downloads/R_Predictive_Modelling_Part2/STA380-master/STA380-master/data")
knitr::opts_chunk$set(echo = TRUE)
data_1<-read.csv("greenbuildings.csv")
setwd("C:/Users/Avani/Downloads/R_Predictive_Modelling_Part2/STA380-master/STA380-master/data")
data_1<-read.csv("greenbuildings.csv")
setwd("C:/Users/Avani/Downloads/R_Predictive_Modelling_Part2/STA380-master/STA380-master/data")
data_1<-read.csv("greenbuildings.csv")
data_1<-read.csv("greenbuildings.csv")
data_1<-read.csv("../data/greenbuildings.csv")
data_1<-read.csv("../data/greenbuildings.csv")
setwd("C:/Users/Avani/Downloads/R_Predictive_Modelling_Part2/STA380-master/STA380-master/data/data")
data_1<-read.csv("../data/greenbuildings.csv")
data_1<-read.csv("greenbuildings.csv")
setwd("C:/Users/Avani/Downloads/R_Predictive_Modelling_Part2/STA380-master/STA380-master/data/data")
data_1<-read.csv("greenbuildings.csv")
setwd("C:/Users/Avani/Downloads/R_Predictive_Modelling_Part2/STA380-master/STA380-master/data")
data_1<-read.csv("../data/greenbuildings.csv")
data_1<-read.csv("../data/greenbuildings.csv")
setwd("C:/Users/Avani/Downloads/R_Predictive_Modelling_Part2/STA380-master/STA380-master/data/data")
